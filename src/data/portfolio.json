{
  "name": "Harikrishna Sureshkumar Pillai",
  "location": "Mississauga, ON, Canada",
  "role": "Business Systems Analyst",
  "specialization": "AI & Data Platforms",
  "positioning": "I work at the intersection of business workflows, data platforms, and AI systems in regulated environments.",
  "socials": {
    "linkedin": "https://linkedin.com/in/harikrishnapillai",
    "email": "spillaiharikrishna@gmail.com"
  },
  "summary": "Business Systems Analyst with 5+ years of experience supporting enterprise data and platform initiatives in regulated financial environments. Experienced in translating business workflows and operational needs into clear, scalable requirements for cloud-based data and analytics platforms across Azure and Databricks. Regularly partners with engineering, data, risk, and compliance teams to ensure solutions are reliable, auditable, and fit for production use. Increasingly involved in initiatives where data products and platforms are designed to support AI and advanced analytics use cases, with a focus on data readiness, governance, and responsible usage.",
  "about": {
    "philosophy": "I am energized by problems that require both technical precision and strategic clarity. In regulated environments, 'good enough' is never enough—systems must be correct, auditable, and scalable. I like to work at the edge of emerging technology (like AI readiness) while maintaining the grounded stability that enterprise operations demand.",
    "howIWork": "I believe in systems thinking. Whether it's a SQL optimization or a platform migration, I start with the 'why' and ensure the technical implementation serves the business goal without compromising on compliance or data integrity."
  },
  "experience": [
    {
      "company": "CIBC",
      "role": "Consultant, Business System Analysis",
      "period": "May 2024 – Present",
      "narrative": "At CIBC, I lead the analysis and delivery of enterprise data and AI readiness initiatives, coordinating priorities across Engineering, Risk, Compliance, and Business stakeholders.",
      "bullets": [
        "Analyzed large, multi-source datasets using SQL and Excel to identify performance drivers and data quality issues supporting executive decision-making.",
        "Supported resource prioritization and roadmap planning by providing data-backed recommendations and intake estimates.",
        "Standardized data governance and metadata practices across 15+ stakeholder groups, improving compliance and usability.",
        "Delivered 30+ enterprise initiatives by coordinating complex requirements between technical teams and regulatory stakeholders.",
        "Recipient of three CIBC Purpose 250 Awards for contributions to enterprise data and regulatory reporting.",
        "Mentored and onboarded 5 new analysts, reducing ramp-up time and improving delivery consistency.",
        "Supported QA in UAT, deployment, and test planning for cloud & AI/ML platforms.",
        "Completed 55+ detailed project intake estimations for executive-level prioritization."
      ]
    },
    {
      "company": "CIBC",
      "role": "Junior Business System Analyst",
      "period": "April 2022 – May 2024",
      "narrative": "Acted as a strategic liaison to deliver enterprise-scale data, cloud, and AI initiatives while supporting regulatory reviews.",
      "bullets": [
        "Strategic liaison between Engineering, Data Platform teams, and multiple business units for cloud and AI initiatives.",
        "Provided executive-ready insights and documentation to support senior leadership decision-making.",
        "Identified technology gaps across reporting workflows and enabled cloud-native solutions for improved audit readiness."
      ]
    },
    {
      "company": "BSC Corp",
      "role": "Process Analyst",
      "period": "August 2020 – September 2021",
      "narrative": "Focused on product quality and operational efficiency through data-driven monitoring and audit controls.",
      "bullets": [
        "Ensured product quality by testing and troubleshooting consumer electronics to meet compliance standards.",
        "Maintained asset and inventory management records using Microsoft Access with strict audit controls.",
        "Improved team performance by 25% by designing Power BI dashboards and analyzing daily targets using SQL and Excel."
      ]
    }
  ],
  "projects": [
    {
      "slug": "lockedin-ios",
      "title": "LockedIn: Personal Productivity & Study Logger",
      "oneLine": "A native iOS application for high-performance habit tracking and study analytics.",
      "problem": "Building a privacy-first, cloud-synced productivity tool that balances native performance with complex backend coordination and monetization constraints.",
      "approach": "Developed using SwiftUI with a state-driven architecture for session management. Integrated Supabase for cross-device sync and RevenueCat for a compliant monetization model.",
      "constraints": "Navigated rigorous App Store Review guidelines for data transparency and account deletion. Optimized a 7MB footprint while supporting complex data schemas.",
      "outcome": "Successfully launched on the App Store. Established a production-ready iOS codebase with automated launch flows and secure, scalable backend integration.",
      "link": "https://apps.apple.com/us/app/lockedin-study-logger/id6756859046",
      "linkText": "View on App Store",
      "skills": [
        "SwiftUI",
        "Supabase",
        "RevenueCat",
        "App Store Connect",
        "iOS Development"
      ]
    },
    {
      "slug": "azure-data-engineering",
      "title": "Azure Data Engineering: End-to-End Batch & Event-Driven Pipelines",
      "oneLine": "A comprehensive lakehouse architecture using ADF, Databricks, and Delta Lake.",
      "image": "/azure-flow.png",
      "problem": "Developing a robust, scalable, and governed data platform to handle diverse data sources while ensuring regulatory compliance and operational cost-efficiency.",
      "approach": "Designed a multi-phase architecture leveraging Azure Data Factory for orchestration and Databricks for processing. The system follows a Bronze-Silver layering strategy for data cleanliness.",
      "sections": [
        {
          "title": "Phase 1: Batch Ingestion & Storage Architecture",
          "content": "Built an ingestion layer to landed raw data from multi-source systems (SQL, APIs, SFTP) into an ADLS Gen2 Bronze container. This decoupling of orchestration (ADF) from processing (Databricks) ensures high availability and scalability.",
          "bullets": [
            "Configured ADF Copy Data activities for diverse protocols.",
            "Established ADLS Gen2 folder structures following lakehouse best practices.",
            "Implemented initial schema validation at the ingestion threshold."
          ]
        },
        {
          "title": "Phase 2: Delta Lake Transformation & Governance",
          "content": "Utilized Spark to clean and standardize data into a curated Silver layer. The transition to Delta Lake provided ACID compliance and versioning, while Unity Catalog enabled centralized governance and secure access control.",
          "bullets": [
            "Implemented Bronze-to-Silver Spark notebooks with deduplication logic.",
            "Integrated Unity Catalog for metastore and permission management.",
            "Optimized performance through Spark partitioning and Z-Ordering."
          ]
        },
        {
          "title": "Phase 3: Event-Driven & Secure Orchestration",
          "content": "Refined the pipeline to run based on data availability rather than static schedules. Parameterized notebooks and dynamic ADF triggers reduced compute costs and improved maintainability.",
          "bullets": [
            "Built event-driven triggers using ADF Get Metadata activities.",
            "Enabled secure, reusable parameter passing from ADF to Databricks.",
            "Refactored notebooks to remove hard-coded dependencies."
          ]
        }
      ],
      "constraints": "Managing schema drift across sources, ensuring Unity Catalog permission alignment, and preventing unnecessary compute spend during idle periods.",
      "outcome": "Delivered a production-ready data platform that reduced processing failures by 30% and improved data auditability by 100% through Delta Lake history.",
      "skills": [
        "Azure Data Factory",
        "Databricks",
        "Delta Lake",
        "Unity Catalog",
        "Spark",
        "ADLS Gen2",
        "Data Governance"
      ]
    },
    {
      "slug": "linux-sql-monitor",
      "title": "Linux Cluster Resource Monitoring App",
      "oneLine": "Bash-based monitoring agent for real-time resource tracking.",
      "problem": "Developing a monitoring agent to keep an active track of resource usage statistics of a node in a network.",
      "approach": "Created Bash scripts to collect metrics, stored in a PostgreSQL container via Docker. Used crontab for automation and SQL for data analysis.",
      "constraints": "Required robust error detection for crontab updates and efficient resource usage on CentOS 7 VMs.",
      "outcome": "Successfully deployed on GCP, providing real-time insights into cluster health.",
      "skills": [
        "Bash",
        "Linux",
        "PostgreSQL",
        "Docker",
        "GCP",
        "Crontab"
      ]
    },
    {
      "slug": "twitter-crd",
      "title": "Twitter CRD App",
      "oneLine": "CLI tool for managing tweets via Twitter REST API.",
      "problem": "Managing Twitter interactions through a command-line interface with proper architectural separation.",
      "approach": "Developed using Java with MVC design pattern. Integrated with Twitter REST APIs using Spring and Maven.",
      "constraints": "Ensuring API rate limit compliance and robust unit testing with Mockito.",
      "outcome": "Production-ready CLI tool deployed via Docker Hub with full test coverage.",
      "skills": [
        "Java",
        "Spring",
        "REST API",
        "MVC",
        "Docker",
        "Mockito"
      ]
    },
    {
      "slug": "java-grep",
      "title": "Java Grep Implementation",
      "oneLine": "Recursive regex search tool reimplemented in Java.",
      "problem": "Recreating the powerful Unix grep utility in Java while optimizing for memory and speed.",
      "approach": "Leveraged Java 8 Lambda and Stream APIs for declarative processing. Built with Maven and containerized with Docker.",
      "constraints": "Handling large directory recursions efficiently without memory overflows.",
      "outcome": "High-performance search utility with a modern functional programming approach.",
      "skills": [
        "Java",
        "Lambda",
        "Stream API",
        "Regex",
        "Maven",
        "Docker"
      ]
    },
    {
      "slug": "retail-analytics-python",
      "title": "Retail Sales Data Analytics",
      "oneLine": "End-to-end data analysis and customer segmentation.",
      "problem": "Transforming raw retail sales data into actionable marketing and budgeting strategies.",
      "approach": "Built a mock OLAP environment using Docker and PostgreSQL. Analyzed data in Jupyter using Pandas and RFM segmentation.",
      "constraints": "Bridging the gap between raw SQL storage and Python-based visualization layers.",
      "outcome": "Detailed RFM segmentation model used to drive customer-centric marketing decisions.",
      "skills": [
        "Python",
        "Pandas",
        "PostgreSQL",
        "Docker",
        "Jupyter",
        "RFM Analysis"
      ]
    },
    {
      "slug": "parking-detection-raspberry",
      "title": "Real-Time Parking Detection System",
      "oneLine": "YOLO-based vision system for parking space monitoring.",
      "problem": "Detecting parking availability in real-time using edge computing and computer vision.",
      "approach": "Deployed YOLO v3 on Raspberry Pi with Intel Neural Compute Stick. Synced data to Firebase for a Flutter web frontend.",
      "constraints": "Managing latency on edge hardware and ensuring real-time Firebase synchronization.",
      "outcome": "Functional IoT system with a real-time map interface for parking users.",
      "skills": [
        "Raspberry Pi",
        "YOLO",
        "Computer Vision",
        "Firebase",
        "Flutter",
        "Python"
      ]
    },
    {
      "slug": "java-jdbc-app",
      "title": "Java JDBC CRUD Application",
      "oneLine": "Database management tool using DAO pattern.",
      "problem": "Creating a reliable interface for PostgreSQL database operations using standard Java APIs.",
      "approach": "Built using JDBC API and the DAO (Data Access Object) design pattern for clean separation of concerns. Managed with Maven.",
      "constraints": "Ensuring proper connection pooling and transaction integrity in a CLI environment.",
      "outcome": "Standardized tool for performing CRUD operations on relational data structures.",
      "skills": [
        "Java",
        "JDBC",
        "PostgreSQL",
        "DAO Pattern",
        "Maven"
      ]
    },
    {
      "slug": "legacy-portfolio",
      "title": "Legacy Responsive Portfolio",
      "oneLine": "Initial web presence built with standard web technologies.",
      "problem": "Designing and deploying a professional web presence from scratch using AWS infrastructure.",
      "approach": "Developed using HTML, CSS, and Bootstrap. Integrated MoveTo.js for smooth scrolling and MailToUI for contact features.",
      "constraints": "Configuring S3 static hosting and Route 53 DNS for high availability.",
      "outcome": "Successfully hosted and maintained a high-traffic personal site (www.analysthari.com).",
      "skills": [
        "HTML",
        "CSS",
        "Bootstrap",
        "AWS S3",
        "AWS Route 53",
        "JavaScript"
      ]
    },
    {
      "slug": "technical-explorations",
      "title": "Big Data & DevOps Explorations",
      "oneLine": "Lab-based projects in Spark, Hadoop, and Spring Boot.",
      "problem": "Understanding the architectural nuances of distributed computing and microservices.",
      "approach": "Series of technical labs and mini-projects focusing on Hadoop HDFS, Spark RDDs, and Spring Boot service orchestration.",
      "outcome": "Developed a strong foundation in distributed system architecture and modern Java backend development.",
      "skills": [
        "Spark",
        "Hadoop",
        "Spring Boot",
        "Cloud DevOps"
      ]
    },
    {
      "slug": "jarvis-data-eng",
      "title": "Jarvis Data Engineering Portfolio",
      "oneLine": "Core technical repository for Data Engineering projects.",
      "problem": "Maintaining a centralized, auditable record of engineering projects across diverse tech stacks.",
      "approach": "A curated collection of projects focusing on Java, SQL, Linux, and Cloud infrastructure.",
      "outcome": "Serves as the primary technical showcase for Jarvis Consulting initiatives.",
      "link": "https://github.com/jarviscanada/jarvis_data_eng_HarikrishnaPillai",
      "linkText": "View Full Repository",
      "skills": [
        "Data Engineering",
        "Java",
        "SQL",
        "Linux",
        "GitHub"
      ]
    }
  ],
  "skills": {
    "Technology & Platforms": [
      "Azure (ADF, ADLS Gen2, IAM)",
      "Databricks (Unity Catalog, DLT)",
      "SwiftUI",
      "iOS Development",
      "LLMs",
      "SQL",
      "Python",
      "Prompt Engineering",
      "R",
      "AI/ML Platforms",
      "REST APIs",
      "n8n"
    ],
    "Business & Strategy": [
      "Technology Business Development",
      "Stakeholder Management",
      "Technology Advisory",
      "Solution Evaluation",
      "Gap Analysis",
      "Commercial Value Assessment"
    ],
    "Data & Governance": [
      "Enterprise Data Platforms",
      "Regulatory Reporting (OSFI, FINTRAC, EFT)",
      "Metadata Management",
      "Data Governance",
      "Compliance",
      "Responsible AI & Ethics"
    ],
    "Delivery & Execution": [
      "Agile & Waterfall",
      "Project Intake & Estimation",
      "Cross-Functional Coordination",
      "Executive Reporting"
    ],
    "Tools": [
      "Git",
      "Confluence",
      "Visual Studio Code",
      "MS Suite",
      "JIRA",
      "Visio",
      "ServiceNow"
    ],
    "Requirements & Delivery Artifacts": [
      "User Stories",
      "BRDs",
      "SRDs",
      "IRDs",
      "STMs",
      "Interface Agreements (IAs)"
    ]
  },
  "certifications": [
    {
      "category": "Certifications",
      "items": [
        {
          "name": "Microsoft Azure AI Fundamentals (AI-900)",
          "issuer": "Microsoft",
          "logo": "microsoft",
          "credentialId": "A9B2-5X1Z",
          "issuedDate": "Feb 2023",
          "expiryDate": ""
        },
        {
          "name": "Microsoft Azure Data Fundamentals (DP-900)",
          "issuer": "Microsoft",
          "logo": "microsoft",
          "credentialId": "I1V3-8P5M",
          "issuedDate": "Nov 2022",
          "expiryDate": ""
        },
        {
          "name": "Scrum Fundamentals Certified (SFC)",
          "issuer": "SCRUMstudy",
          "logo": "scrum",
          "credentialId": "",
          "issuedDate": "",
          "expiryDate": ""
        },
        {
          "name": "AI Ethics, Responsible Use, and Creativity",
          "issuer": "University of Michigan (Coursera)",
          "logo": "university",
          "credentialId": "",
          "issuedDate": "",
          "expiryDate": ""
        }
      ]
    }
  ],
  "education": [
    {
      "degree": "Post-Graduate Diploma, Electronics Engineering Technology",
      "institution": "Centennial College, Toronto",
      "year": "2018 – 2020"
    },
    {
      "degree": "Bachelor of Technology, Electronics and Communication Engineering",
      "institution": "Mahatma Gandhi University, India",
      "year": "2012 – 2016"
    }
  ]
}