{
  "name": "Harikrishna Sureshkumar Pillai",
  "location": "Mississauga, ON, Canada",
  "role": "Business Systems Analyst",
  "specialization": "AI & Data Platforms",
  "positioning": "I work at the intersection of business workflows, data platforms, and AI systems in regulated environments.",
  "socials": {
    "linkedin": "https://linkedin.com/in/harikrishnapillai",
    "email": "spillaiharikrishna@gmail.com"
  },
  "summary": "Business Systems Analyst with 5+ years of experience supporting enterprise data and platform initiatives in regulated financial environments. Experienced in translating business workflows and operational needs into clear, scalable requirements for cloud-based data and analytics platforms across Azure and Databricks. Regularly partners with engineering, data, risk, and compliance teams to ensure solutions are reliable, auditable, and fit for production use. Increasingly involved in initiatives where data products and platforms are designed to support AI and advanced analytics use cases, with a focus on data readiness, governance, and responsible usage.",
  "about": {
    "philosophy": "I am energized by problems that require both technical precision and strategic clarity. In regulated environments, 'good enough' is never enough—systems must be correct, auditable, and scalable. I like to work at the edge of emerging technology (like AI readiness) while maintaining the grounded stability that enterprise operations demand.",
    "howIWork": "I believe in systems thinking. Whether it's a SQL optimization or a platform migration, I start with the 'why' and ensure the technical implementation serves the business goal without compromising on compliance or data integrity."
  },
  "experience": [
    {
      "company": "CIBC",
      "role": "Consultant, Business System Analysis",
      "period": "May 2024 – Present",
      "narrative": "At CIBC, I lead the analysis and delivery of enterprise data and AI readiness initiatives, coordinating priorities across Engineering, Risk, Compliance, and Business stakeholders.",
      "bullets": [
        "Analyzed large, multi-source datasets using SQL and Excel to identify performance drivers and data quality issues supporting executive decision-making.",
        "Supported resource prioritization and roadmap planning by providing data-backed recommendations and intake estimates.",
        "Standardized data governance and metadata practices across 15+ stakeholder groups, improving compliance and usability.",
        "Delivered 30+ enterprise initiatives by coordinating complex requirements between technical teams and regulatory stakeholders.",
        "Recipient of three CIBC Purpose 250 Awards for contributions to enterprise data and regulatory reporting.",
        "Mentored and onboarded 5 new analysts, reducing ramp-up time and improving delivery consistency.",
        "Supported QA in UAT, deployment, and test planning for cloud & AI/ML platforms.",
        "Completed 55+ detailed project intake estimations for executive-level prioritization."
      ]
    },
    {
      "company": "CIBC",
      "role": "Junior Business System Analyst",
      "period": "April 2022 – May 2024",
      "narrative": "Acted as a strategic liaison to deliver enterprise-scale data, cloud, and AI initiatives while supporting regulatory reviews.",
      "bullets": [
        "Strategic liaison between Engineering, Data Platform teams, and multiple business units for cloud and AI initiatives.",
        "Provided executive-ready insights and documentation to support senior leadership decision-making.",
        "Identified technology gaps across reporting workflows and enabled cloud-native solutions for improved audit readiness."
      ]
    },
    {
      "company": "BSC Corp",
      "role": "Process Analyst",
      "period": "August 2020 – September 2021",
      "narrative": "Focused on product quality and operational efficiency through data-driven monitoring and audit controls.",
      "bullets": [
        "Ensured product quality by testing and troubleshooting consumer electronics to meet compliance standards.",
        "Maintained asset and inventory management records using Microsoft Access with strict audit controls.",
        "Improved team performance by 25% by designing Power BI dashboards and analyzing daily targets using SQL and Excel."
      ]
    }
  ],
  "projects": [
    {
      "slug": "lockedin-ios",
      "title": "LockedIn: Personal Productivity & Study Logger",
      "oneLine": "A native iOS application for high-performance habit tracking and study analytics.",
      "problem": "Building a privacy-first, cloud-synced productivity tool that balances native performance with complex backend coordination and monetization constraints.",
      "approach": "Developed using SwiftUI with a state-driven architecture for session management. Integrated Supabase for cross-device sync and RevenueCat for a compliant monetization model.",
      "constraints": "Navigated rigorous App Store Review guidelines for data transparency and account deletion. Optimized a 7MB footprint while supporting complex data schemas.",
      "outcome": "Successfully launched on the App Store. Established a production-ready iOS codebase with automated launch flows and secure, scalable backend integration.",
      "link": "https://apps.apple.com/us/app/lockedin-study-logger/id6756859046",
      "linkText": "View on App Store",
      "skills": [
        "SwiftUI",
        "Supabase",
        "RevenueCat",
        "App Store Connect",
        "iOS Development"
      ],
      "category": "Mobile & AI"
    },
    {
      "slug": "azure-data-engineering",
      "title": "Azure Data Engineering: End-to-End Batch & Event-Driven Pipelines",
      "oneLine": "A comprehensive lakehouse architecture using ADF, Databricks, and Delta Lake.",
      "image": "/azure-flow.png",
      "problem": "Developing a robust, scalable, and governed data platform to handle diverse data sources while ensuring regulatory compliance and operational cost-efficiency.",
      "approach": "Designed a multi-phase architecture leveraging Azure Data Factory for orchestration and Databricks for processing. The system follows a Bronze-Silver layering strategy for data cleanliness.",
      "sections": [
        {
          "title": "Phase 1: Batch Ingestion & Storage Architecture",
          "content": "Built an ingestion layer to landed raw data from multi-source systems (SQL, APIs, SFTP) into an ADLS Gen2 Bronze container. This decoupling of orchestration (ADF) from processing (Databricks) ensures high availability and scalability.",
          "bullets": [
            "Configured ADF Copy Data activities for diverse protocols.",
            "Established ADLS Gen2 folder structures following lakehouse best practices.",
            "Implemented initial schema validation at the ingestion threshold."
          ]
        },
        {
          "title": "Phase 2: Delta Lake Transformation & Governance",
          "content": "Utilized Spark to clean and standardize data into a curated Silver layer. The transition to Delta Lake provided ACID compliance and versioning, while Unity Catalog enabled centralized governance and secure access control.",
          "bullets": [
            "Implemented Bronze-to-Silver Spark notebooks with deduplication logic.",
            "Integrated Unity Catalog for metastore and permission management.",
            "Optimized performance through Spark partitioning and Z-Ordering."
          ]
        },
        {
          "title": "Phase 3: Event-Driven & Secure Orchestration",
          "content": "Refined the pipeline to run based on data availability rather than static schedules. Parameterized notebooks and dynamic ADF triggers reduced compute costs and improved maintainability.",
          "bullets": [
            "Built event-driven triggers using ADF Get Metadata activities.",
            "Enabled secure, reusable parameter passing from ADF to Databricks.",
            "Refactored notebooks to remove hard-coded dependencies."
          ]
        }
      ],
      "constraints": "Managing schema drift across sources, ensuring Unity Catalog permission alignment, and preventing unnecessary compute spend during idle periods.",
      "outcome": "Delivered a production-ready data platform that reduced processing failures by 30% and improved data auditability by 100% through Delta Lake history.",
      "skills": [
        "Azure Data Factory",
        "Databricks",
        "Delta Lake",
        "Unity Catalog",
        "Spark",
        "ADLS Gen2",
        "Data Governance"
      ],
      "category": "Data & Architecture"
    },
    {
      "slug": "linux-sql-monitor",
      "title": "Linux Cluster Resource Monitoring App",
      "oneLine": "A robust monitoring agent built with Bash and PostgreSQL for real-time node resource tracking.",
      "problem": "Maintaining visibility into resource usage across a network of nodes is critical for cluster health. The challenge was to create a lightweight agent that could track metrics without overhead on the host VMs.",
      "approach": "Developed a suite of Bash scripts that leverage Linux utilities to capture CPU, memory, and disk statistics. These metrics are formatted and loaded into a Dockerized PostgreSQL environment. Used crontab to automate the monitoring pulse and built SQL analytical queries to detect performance anomalies.",
      "constraints": "Required a small memory footprint on host CentOS 7 machines. Implemented error detection queries to ensure the monitoring agent remained active and reporting to the central database without gaps.",
      "outcome": "Successfully deployed on Google Cloud Platform (GCP), providing an automated, auditable record of resource usage that reduced manual cluster health checks by 80%.",
      "skills": [
        "Bash",
        "Linux",
        "PostgreSQL",
        "Docker",
        "GCP",
        "Crontab",
        "SQL Analysis"
      ],
      "category": "Systems & DevOps"
    },
    {
      "slug": "retail-analytics-python",
      "title": "Retail Sales Data Analytics",
      "oneLine": "End-to-end analytics platform for customer segmentation and marketing strategy optimization.",
      "problem": "Transforming siloed retail sales data from clients (LGS) into actionable insights for marketing and budgeting strategies.",
      "approach": "Provisioned a PostgreSQL database via Docker to host raw data. Used Jupyter Notebooks to establish a data connection and employed Pandas for cleaning and exploratory data analysis. Crucially, I integrated an RFM (Recency, Frequency, Monetary) segmentation model to categorize customer behavior.",
      "constraints": "Managing data types across the SQL-to-Python bridge and ensuring the segmentation logic aligned with the client's commercial objectives.",
      "outcome": "The RFM model identified high-value clusters and at-risk customers, directly informing a 15% improvement in marketing budget allocation strategies.",
      "skills": [
        "Python",
        "Pandas",
        "PostgreSQL",
        "Docker",
        "Jupyter",
        "RFM Segmentation",
        "Data Visualization"
      ],
      "category": "Data & Architecture"
    },
    {
      "slug": "parking-detection-raspberry",
      "title": "Real-Time Parking Detection System",
      "oneLine": "An IoT vision system utilizing edge computing for real-time monitoring of parking availability.",
      "problem": "Legacy parking systems are often analog or laggy. The project aimed to provide instant, visually-verified availability updates to motorists.",
      "approach": "Deployed the YOLO v3 (You Only Look Once) object detection algorithm on a Raspberry Pi accelerated by an Intel Neural Compute Stick. The system processes Pi camera feeds in real-time, syncing spot statuses to Firebase. A Flutter-based web app consumed this data to provide a live GUI with Google Maps integration.",
      "constraints": "Optimizing deep learning models for edge hardware and ensuring low-latency updates between the Pi and the Firebase real-time database.",
      "outcome": "Developed a fully functional prototype that tracked parking occupancy with 95% accuracy and broadcasted updates in under 2 seconds.",
      "skills": [
        "Raspberry Pi",
        "YOLO Object Detection",
        "Intel Neural Compute Stick",
        "Firebase",
        "Flutter",
        "Computer Vision",
        "IoT"
      ],
      "category": "Mobile & AI"
    },
    {
      "slug": "technical-explorations",
      "title": "Big Data & DevOps Explorations",
      "oneLine": "A series of laboratory researches into distributed computing architectures and service orchestration.",
      "problem": "Bridging the gap between monolithic application logic and the distributed requirements of big data platforms.",
      "approach": "Completed deep-dives into Hadoop HDFS for distributed storage, Spark for in-memory processing, and Spring Boot for microservice development. Focused on HDFS data replication and Spark RDD transformations.",
      "outcome": "Gained specialized knowledge in high-concurrency systems which I now apply to CIBC's enterprise data platforms.",
      "skills": [
        "Apache Spark",
        "Hadoop",
        "Spring Boot",
        "Distributed Systems",
        "HDFS",
        "Microservices"
      ],
      "category": "Data & Architecture"
    },
    {
      "slug": "jarvis-data-eng",
      "title": "Jarvis Data Engineering Portfolio",
      "oneLine": "Core technical repository for Data Engineering projects.",
      "problem": "Maintaining a centralized, auditable record of engineering projects across diverse tech stacks.",
      "approach": "A curated collection of projects focusing on Java, SQL, Linux, and Cloud infrastructure.",
      "outcome": "Serves as the primary technical showcase for Jarvis Consulting initiatives.",
      "link": "https://github.com/jarviscanada/jarvis_data_eng_HarikrishnaPillai",
      "linkText": "View Full Repository",
      "skills": [
        "Data Engineering",
        "Java",
        "SQL",
        "Linux",
        "GitHub"
      ],
      "category": "Data & Architecture"
    }
  ],
  "skills": {
    "Technology & Platforms": [
      "Azure (ADF, ADLS Gen2, IAM)",
      "Databricks (Unity Catalog, DLT)",
      "SwiftUI",
      "iOS Development",
      "LLMs",
      "SQL",
      "Python",
      "Prompt Engineering",
      "R",
      "AI/ML Platforms",
      "REST APIs",
      "n8n"
    ],
    "Business & Strategy": [
      "Technology Business Development",
      "Stakeholder Management",
      "Technology Advisory",
      "Solution Evaluation",
      "Gap Analysis",
      "Commercial Value Assessment"
    ],
    "Data & Governance": [
      "Enterprise Data Platforms",
      "Regulatory Reporting (OSFI, FINTRAC, EFT)",
      "Metadata Management",
      "Data Governance",
      "Compliance",
      "Responsible AI & Ethics"
    ],
    "Delivery & Execution": [
      "Agile & Waterfall",
      "Project Intake & Estimation",
      "Cross-Functional Coordination",
      "Executive Reporting"
    ],
    "Tools": [
      "Git",
      "Confluence",
      "Visual Studio Code",
      "MS Suite",
      "JIRA",
      "Visio",
      "ServiceNow"
    ],
    "Requirements & Delivery Artifacts": [
      "User Stories",
      "BRDs",
      "SRDs",
      "IRDs",
      "STMs",
      "Interface Agreements (IAs)"
    ]
  },
  "certifications": [
    {
      "category": "Certifications",
      "items": [
        {
          "name": "Microsoft Azure AI Fundamentals (AI-900)",
          "issuer": "Microsoft",
          "logo": "microsoft",
          "credentialId": "A9B2-5X1Z",
          "issuedDate": "Feb 2023",
          "expiryDate": ""
        },
        {
          "name": "Microsoft Azure Data Fundamentals (DP-900)",
          "issuer": "Microsoft",
          "logo": "microsoft",
          "credentialId": "I1V3-8P5M",
          "issuedDate": "Nov 2022",
          "expiryDate": ""
        },
        {
          "name": "Scrum Fundamentals Certified (SFC)",
          "issuer": "SCRUMstudy",
          "logo": "scrum",
          "credentialId": "",
          "issuedDate": "",
          "expiryDate": ""
        },
        {
          "name": "AI Ethics, Responsible Use, and Creativity",
          "issuer": "University of Michigan (Coursera)",
          "logo": "university",
          "credentialId": "",
          "issuedDate": "",
          "expiryDate": ""
        }
      ]
    }
  ],
  "education": [
    {
      "degree": "Post-Graduate Diploma, Electronics Engineering Technology",
      "institution": "Centennial College, Toronto",
      "year": "2018 – 2020"
    },
    {
      "degree": "Bachelor of Technology, Electronics and Communication Engineering",
      "institution": "Mahatma Gandhi University, India",
      "year": "2012 – 2016"
    }
  ]
}