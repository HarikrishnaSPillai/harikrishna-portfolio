{
  "name": "Harikrishna Sureshkumar Pillai",
  "location": "Mississauga, ON, Canada",
  "role": "Business Systems Analyst",
  "specialization": "AI & Data Platforms",
  "positioning": "I work at the intersection of business workflows, data platforms, and AI systems in regulated environments.",
  "socials": {
    "linkedin": "https://linkedin.com/in/harikrishnapillai",
    "email": "spillaiharikrishna@gmail.com"
  },
  "summary": "Business Systems Analyst with 5+ years of experience supporting enterprise data and platform initiatives in regulated financial environments. Experienced in translating business workflows and operational needs into clear, scalable requirements for cloud-based data and analytics platforms across Azure and Databricks. Regularly partners with engineering, data, risk, and compliance teams to ensure solutions are reliable, auditable, and fit for production use. Increasingly involved in initiatives where data products and platforms are designed to support AI and advanced analytics use cases, with a focus on data readiness, governance, and responsible usage.",
  "about": {
    "philosophy": "I am energized by problems that require both technical precision and strategic clarity. In regulated environments, 'good enough' is never enough—systems must be correct, auditable, and scalable. I like to work at the edge of emerging technology (like AI readiness) while maintaining the grounded stability that enterprise operations demand.",
    "howIWork": "I believe in systems thinking. Whether it's a SQL optimization or a platform migration, I start with the 'why' and ensure the technical implementation serves the business goal without compromising on compliance or data integrity."
  },
  "experience": [
    {
      "company": "CIBC",
      "role": "Consultant, Business System Analysis",
      "period": "May 2024 – Present",
      "narrative": "At CIBC, I lead the analysis and delivery of enterprise data and AI readiness initiatives, coordinating priorities across Engineering, Risk, Compliance, and Business stakeholders.",
      "bullets": [
        "Analyzed large, multi-source datasets using SQL and Excel to identify performance drivers and data quality issues supporting executive decision-making.",
        "Supported resource prioritization and roadmap planning by providing data-backed recommendations and intake estimates.",
        "Standardized data governance and metadata practices across 15+ stakeholder groups, improving compliance and usability.",
        "Delivered 30+ enterprise initiatives by coordinating complex requirements between technical teams and regulatory stakeholders.",
        "Recipient of three CIBC Purpose 250 Awards for contributions to enterprise data and regulatory reporting.",
        "Mentored and onboarded 5 new analysts, reducing ramp-up time and improving delivery consistency.",
        "Supported QA in UAT, deployment, and test planning for cloud & AI/ML platforms.",
        "Completed 55+ detailed project intake estimations for executive-level prioritization."
      ]
    },
    {
      "company": "CIBC",
      "role": "Junior Business System Analyst",
      "period": "April 2022 – May 2024",
      "narrative": "Acted as a strategic liaison to deliver enterprise-scale data, cloud, and AI initiatives while supporting regulatory reviews.",
      "bullets": [
        "Strategic liaison between Engineering, Data Platform teams, and multiple business units for cloud and AI initiatives.",
        "Provided executive-ready insights and documentation to support senior leadership decision-making.",
        "Identified technology gaps across reporting workflows and enabled cloud-native solutions for improved audit readiness."
      ]
    },
    {
      "company": "BSC Corp",
      "role": "Process Analyst",
      "period": "August 2020 – September 2021",
      "narrative": "Focused on product quality and operational efficiency through data-driven monitoring and audit controls.",
      "bullets": [
        "Ensured product quality by testing and troubleshooting consumer electronics to meet compliance standards.",
        "Maintained asset and inventory management records using Microsoft Access with strict audit controls.",
        "Improved team performance by 25% by designing Power BI dashboards and analyzing daily targets using SQL and Excel."
      ]
    }
  ],
  "projects": [
    {
      "slug": "lockedin-ios",
      "title": "LockedIn: Personal Productivity & Study Logger",
      "oneLine": "A native iOS application for high-performance habit tracking and study analytics.",
      "problem": "Building a privacy-first, cloud-synced productivity tool that balances native performance with complex backend coordination and monetization constraints.",
      "approach": "Developed using SwiftUI with a state-driven architecture for session management. Integrated Supabase for cross-device sync and RevenueCat for a compliant monetization model.",
      "constraints": "Navigated rigorous App Store Review guidelines for data transparency and account deletion. Optimized a 7MB footprint while supporting complex data schemas.",
      "outcome": "Successfully launched on the App Store. Established a production-ready iOS codebase with automated launch flows and secure, scalable backend integration.",
      "link": "https://apps.apple.com/us/app/lockedin-study-logger/id6756859046",
      "linkText": "View on App Store",
      "skills": [
        "SwiftUI",
        "Supabase",
        "RevenueCat",
        "App Store Connect",
        "iOS Development"
      ]
    },
    {
      "slug": "azure-batch-ingestion",
      "title": "End-to-End Batch Data Ingestion & Transformation (Azure + Databricks)",
      "oneLine": "Building a basic but production-style data pipeline on Azure.",
      "problem": "I built my first end-to-end batch pipeline using Azure Data Factory, ADLS Gen2, and Databricks to understand how raw data moves from ingestion to transformation in a cloud environment.",
      "approach": "Used ADF for orchestration and Databricks for processing. Implemented defensive Spark reads to handle schema inconsistencies and restructured storage into Bronze and Silver layers.",
      "constraints": "Challenges included understanding ADF-Databricks handoff, handling schema drift in raw data, and organizing storage layers.",
      "outcome": "Established a 2-layer lakehouse architecture (Bronze/Silver), decoupled orchestration from processing, and successfully handled imperfect real-world data.",
      "skills": [
        "Azure Data Factory",
        "ADLS Gen2",
        "Databricks",
        "Spark",
        "Batch Processing"
      ]
    },
    {
      "slug": "delta-lake-unity-catalog",
      "title": "Bronze → Silver Delta Lake Pipeline with Unity Catalog",
      "oneLine": "Building a scalable, governed data engineering pipeline using modern lakehouse practices.",
      "problem": "Focused on building a production-style Bronze to Silver pipeline using Delta Lake and Unity Catalog, with governance and data quality in mind.",
      "approach": "Implemented Delta writes with ACID properties, configured Unity Catalog for governance, and added deduplication logic during Silver transformation.",
      "constraints": "Faced challenges with Delta Lake concepts, Unity Catalog permissions/access control, and dirty records in the Bronze layer.",
      "outcome": "A reliable and auditable pipeline with centralized metadata management and improved data quality.",
      "skills": [
        "Delta Lake",
        "Unity Catalog",
        "Databricks",
        "Data Governance",
        "Azure Data Factory"
      ]
    },
    {
      "slug": "event-driven-parameterized-pipeline",
      "title": "Event-Driven & Secure Data Processing Pipeline",
      "oneLine": "Building a realistic enterprise-grade, event-driven data pipeline.",
      "problem": "Simulated an enterprise scenario where pipelines run based on data availability rather than static schedules, ensuring security and reusability.",
      "approach": "Used ADF Get Metadata activities for event-driven triggers. Parameterized both ADF and Databricks to make notebooks reusable and secure.",
      "constraints": "Challenges included preventing unnecessary Databricks runs, passing dynamic parameters securely, and refactoring hard-coded notebooks.",
      "outcome": "Reduced costs through event-driven processing and improved maintainability via modular, enterprise-standard parameterization.",
      "skills": [
        "Event-Driven Ingestion",
        "Secure Parameterization",
        "Modular Notebook Design",
        "Azure Data Factory",
        "Databricks"
      ]
    }
  ],
  "skills": {
    "Technology & Platforms": [
      "Azure (ADF, ADLS Gen2, IAM)",
      "Databricks (Unity Catalog, DLT)",
      "SwiftUI",
      "iOS Development",
      "LLMs",
      "SQL",
      "Python",
      "Prompt Engineering",
      "R",
      "AI/ML Platforms",
      "REST APIs",
      "n8n"
    ],
    "Business & Strategy": [
      "Technology Business Development",
      "Stakeholder Management",
      "Technology Advisory",
      "Solution Evaluation",
      "Gap Analysis",
      "Commercial Value Assessment"
    ],
    "Data & Governance": [
      "Enterprise Data Platforms",
      "Regulatory Reporting (OSFI, FINTRAC, EFT)",
      "Metadata Management",
      "Data Governance",
      "Compliance",
      "Responsible AI & Ethics"
    ],
    "Delivery & Execution": [
      "Agile & Waterfall",
      "Project Intake & Estimation",
      "Cross-Functional Coordination",
      "Executive Reporting"
    ],
    "Tools": [
      "Git",
      "Confluence",
      "Visual Studio Code",
      "MS Suite",
      "JIRA",
      "Visio",
      "ServiceNow"
    ],
    "Requirements & Delivery Artifacts": [
      "User Stories",
      "BRDs",
      "SRDs",
      "IRDs",
      "STMs",
      "Interface Agreements (IAs)"
    ]
  },
  "certifications": [
    {
      "category": "Certifications",
      "items": [
        {
          "name": "Microsoft Azure AI Fundamentals (AI-900)",
          "issuer": "Microsoft",
          "logo": "microsoft",
          "credentialId": "A9B2-5X1Z",
          "issuedDate": "Feb 2023",
          "expiryDate": ""
        },
        {
          "name": "Microsoft Azure Data Fundamentals (DP-900)",
          "issuer": "Microsoft",
          "logo": "microsoft",
          "credentialId": "I1V3-8P5M",
          "issuedDate": "Nov 2022",
          "expiryDate": ""
        },
        {
          "name": "Scrum Fundamentals Certified (SFC)",
          "issuer": "SCRUMstudy",
          "logo": "scrum",
          "credentialId": "",
          "issuedDate": "",
          "expiryDate": ""
        },
        {
          "name": "AI Ethics, Responsible Use, and Creativity",
          "issuer": "University of Michigan (Coursera)",
          "logo": "university",
          "credentialId": "",
          "issuedDate": "",
          "expiryDate": ""
        }
      ]
    }
  ],
  "education": [
    {
      "degree": "Post-Graduate Diploma, Electronics Engineering Technology",
      "institution": "Centennial College, Toronto",
      "year": "2018 – 2020"
    },
    {
      "degree": "Bachelor of Technology, Electronics and Communication Engineering",
      "institution": "Mahatma Gandhi University, India",
      "year": "2012 – 2016"
    }
  ]
}